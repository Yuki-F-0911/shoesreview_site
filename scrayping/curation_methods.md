# 情報キュレーション方法の拡張案

## 既存のキュレーション方法

1. **Web記事スクレイピング** (`crawl.py`)
   - Google検索APIでレビュー記事を検索
   - Trafilaturaで本文抽出
   - LLMで構造化

2. **YouTube動画要約** (`youtube_summarizer.py`)
   - 音声ダウンロード → Whisper文字起こし → Gemini要約

3. **データ統合** (`consolidate_json_to_csv.py`)
   - 複数ソースの統合とグループ化

---

## 追加提案：10のキュレーション方法

### 1. Redditレビュー収集 (`reddit_curator.py`)
**目的**: Redditのr/RunningShoeGeeksなどのコミュニティから実際のユーザーレビューを収集

**実装内容**:
- Reddit API (PRAW) を使用
- 特定のサブレディットから投稿を取得
- コメントスレッドも含めて収集
- 投票数やコメント数でフィルタリング

**メリット**:
- 実際のユーザーの生の声
- 長期的な使用感の情報
- 比較レビューが多い

---

### 2. ECサイトレビュー収集 (`ec_review_curator.py`)
**目的**: Amazon、楽天などのECサイトから商品レビューを収集

**実装内容**:
- Amazon Product Advertising API またはスクレイピング
- レビューの評価、本文、日付を取得
- 星評価の分布を分析
- レビューの信頼性スコアを計算

**メリット**:
- 大量のユーザーレビュー
- 購入者の実際の使用感
- 価格情報も同時に取得可能

---

### 3. 価格追跡システム (`price_tracker.py`)
**目的**: 複数のECサイトで価格変動を追跡し、最安値情報を提供

**実装内容**:
- 定期的な価格チェック（cron job）
- 価格履歴の保存
- 価格アラート機能
- 複数サイトの価格比較

**メリット**:
- ユーザーに価値ある情報を提供
- セール情報の自動収集
- 価格トレンドの可視化

---

### 4. 新製品リリース情報収集 (`release_tracker.py`)
**目的**: ブランド公式サイトやニュースサイトから新製品情報を自動収集

**実装内容**:
- RSSフィードの監視
- ブランド公式サイトのスクレイピング
- ニュースサイトのキーワード監視
- リリース日、価格、スペック情報の抽出

**メリット**:
- 最新情報の自動収集
- サイトの新鮮さを維持
- SEO効果

---

### 5. 画像解析による特徴抽出 (`image_analyzer.py`)
**目的**: シューズの画像から視覚的特徴を抽出

**実装内容**:
- Google Vision API または OpenAI Vision API
- 画像から色、デザイン、特徴を抽出
- 複数画像の比較
- 画像の品質評価

**メリット**:
- 視覚的な情報の構造化
- デザイン比較の自動化
- 画像検索の精度向上

---

### 6. 多言語レビューの翻訳統合 (`multilang_curator.py`)
**目的**: 英語以外のレビューを翻訳して統合

**実装内容**:
- Google Translate API または DeepL API
- 多言語サイトのスクレイピング
- 翻訳品質の評価
- 元の言語情報の保持

**メリット**:
- 情報源の拡大
- グローバルな視点
- より多くのレビューを収集

---

### 7. 時系列データ分析 (`temporal_analyzer.py`)
**目的**: レビューの評価や内容が時間とともにどう変化するかを分析

**実装内容**:
- レビューの日付情報を活用
- 評価スコアの時系列変化
- キーワードの出現頻度の変化
- トレンド分析

**メリット**:
- 製品の進化を追跡
- 長期使用感の把握
- 問題点の早期発見

---

### 8. 競合比較の自動生成 (`comparison_generator.py`)
**目的**: 類似製品の自動比較レポートを生成

**実装内容**:
- カテゴリ、価格帯、スペックで類似製品を特定
- レビューの比較分析
- 比較表の自動生成
- 推奨シナリオの提案

**メリット**:
- ユーザーの意思決定支援
- コンテンツの自動生成
- SEO効果

---

### 9. 専門家レビューの優先度付け (`expert_scorer.py`)
**目的**: レビューソースの信頼性を評価し、優先度を設定

**実装内容**:
- レビューサイトの権威性スコア
- レビュアーの専門性評価
- レビューの詳細度評価
- 重み付け平均の計算

**メリット**:
- 情報の質の向上
- 信頼性の高い情報の優先表示
- バイアスの軽減

---

### 10. ソーシャルメディア監視 (`social_monitor.py`)
**目的**: Twitter/X、Instagramからシューズ関連の投稿を収集

**実装内容**:
- Twitter API v2 を使用
- ハッシュタグやキーワードで検索
- インフルエンサーの投稿を優先
- 画像付き投稿の分析

**メリット**:
- リアルタイムな情報収集
- トレンドの早期把握
- 視覚的な情報の収集

---

## 実装優先度

### 高優先度（すぐに実装可能）
1. **Redditレビュー収集** - APIが使いやすく、価値の高い情報源
2. **価格追跡システム** - ユーザーに直接的な価値を提供
3. **新製品リリース情報収集** - サイトの新鮮さを維持

### 中優先度（中期的に実装）
4. **ECサイトレビュー収集** - 大量のデータを取得可能
5. **多言語レビューの翻訳統合** - 情報源の拡大
6. **時系列データ分析** - 既存データの活用

### 低優先度（長期的に検討）
7. **画像解析** - APIコストが高い
8. **競合比較の自動生成** - 既存データが十分に蓄積されてから
9. **専門家レビューの優先度付け** - 評価基準の確立が必要
10. **ソーシャルメディア監視** - API制限やコストを考慮

---

## 統合アーキテクチャ

すべてのキュレーション方法を統合するための共通インターフェース:

```python
class CurationMethod:
    def collect(self, query: str) -> List[Dict]:
        """情報を収集"""
        pass
    
    def process(self, raw_data: Dict) -> Dict:
        """データを処理・構造化"""
        pass
    
    def save(self, processed_data: Dict) -> str:
        """データを保存"""
        pass
```

各キュレーション方法はこのインターフェースを実装し、統一された形式でデータを出力。

---

## 次のステップ

1. 優先度の高い方法から実装を開始
2. 既存の `crawl.py` と統合
3. データベースへの保存機能を追加
4. 定期実行のスケジューリング
5. エラーハンドリングとログ機能の強化



